{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac097741",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import allel\n",
    "import malariagen_data\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a892ca",
   "metadata": {},
   "source": [
    "### Writing out haplotypes for phylogenetic analysis\n",
    "\n",
    "First, lets set the genome position of our focus, and also set a position for a separate analysis of a downstream and upstream region, to compare it to. We can also set the size of the flanking region in bp, I set this to 10kb either side of the focus (same as Xavi rdl ms).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f346144b",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "region = '2L:2,358,158-2,431,617'\n",
    "\n",
    "region_names = ['focal', 'upstream', 'downstream']\n",
    "regions = [2_422_652, 2_358_158, 2_431_617]\n",
    "\n",
    "locus_name = 'vgsc'\n",
    "contig = '2L'\n",
    "flanking = 10_000\n",
    "out_prefix = \"../fastas/\" \n",
    "\n",
    "sample_sets = ['1177-VO-ML-LEHMANN-VMF00004', '1177-VO-ML-LEHMANN-VMF00015',\n",
    "       '1178-VO-UG-LAWNICZAK-VMF00025',\n",
    "       '1188-VO-NIANG-NIEL-SN-2304-VMF00259',\n",
    "       '1190-VO-GH-AMENGA-ETEGO-VMF00013',\n",
    "       '1190-VO-GH-AMENGA-ETEGO-VMF00014',\n",
    "       '1190-VO-GH-AMENGA-ETEGO-VMF00028',\n",
    "       '1190-VO-GH-AMENGA-ETEGO-VMF00029',\n",
    "       '1190-VO-GH-AMENGA-ETEGO-VMF00046',\n",
    "       '1190-VO-GH-AMENGA-ETEGO-VMF00047',\n",
    "       '1190-VO-GH-AMENGA-ETEGO-VMF00088',\n",
    "       '1190-VO-GH-AMENGA-ETEGO-VMF00102',\n",
    "       '1230-VO-MULTI-AYALA-AYDI-GA-2204',\n",
    "       '1237-VO-BJ-DJOGBENOU-VMF00050', '1237-VO-BJ-DJOGBENOU-VMF00067',\n",
    "       '1237-VO-BJ-DJOGBENOU-VMF00212', '1244-VO-GH-YAWSON-VMF00051',\n",
    "       '1244-VO-GH-YAWSON-VMF00149', '1245-VO-CI-CONSTANT-VMF00054',\n",
    "       '1246-VO-TZ-KABULA-VMF00185', '1246-VO-TZ-KABULA-VMF00197',\n",
    "       '1253-VO-TG-DJOGBENOU-VMF00052', '1264-VO-CD-WATSENGA-VMF00161',\n",
    "       '1264-VO-CD-WATSENGA-VMF00164', '1270-VO-MULTI-PAMGEN-VMF00232',\n",
    "       '1273-VO-ZM-MULEBA-VMF00176', '1274-VO-KE-KAMAU-VMF00246',\n",
    "       '1281-VO-CM-CHRISTOPHE-VMF00208', '1281-VO-CM-CHRISTOPHE-VMF00227',\n",
    "       '1288-VO-UG-DONNELLY-VMF00168', '1288-VO-UG-DONNELLY-VMF00219',\n",
    "       '1296-VO-BF-DIABATE-VMF00272', '1314-VO-BF-KIENTEGA-KIMA-BF-2104',\n",
    "       '1315-VO-NG-OMITOLA-OMOL-NG-2008', '1323-VO-GM-NGWA-VMF00235',\n",
    "       '1323-VO-GM-NGWA-VMF00242', '1326-VO-UG-KAYONDO-KAJO-UG-2203',\n",
    "       '1329-VO-GA-CHRISTOPHE-VMF00228', '1330-VO-GN-LAMA-VMF00250',\n",
    "       '1339-VO-GH-AMENGA-ETEGO-VMF00302', '1351-VO-SS-WEETMAN-VMF00282', \n",
    "        'AG1000G-AO', 'AG1000G-BF-A', 'AG1000G-BF-B', 'AG1000G-BF-C',\n",
    "       'AG1000G-CD', 'AG1000G-CF', 'AG1000G-CI', 'AG1000G-CM-A',\n",
    "       'AG1000G-CM-B', 'AG1000G-CM-C', 'AG1000G-FR', 'AG1000G-GA-A',\n",
    "       'AG1000G-GH', 'AG1000G-GM-A', 'AG1000G-GM-B', 'AG1000G-GM-C',\n",
    "       'AG1000G-GN-A', 'AG1000G-GN-B', 'AG1000G-GQ', 'AG1000G-GW',\n",
    "       'AG1000G-KE', 'AG1000G-ML-A', 'AG1000G-ML-B', 'AG1000G-MW',\n",
    "       'AG1000G-MZ', 'AG1000G-TZ', 'AG1000G-UG',\n",
    "       'barron-2019', 'bergey-2019', 'campos-2021', 'crawford-2016',\n",
    "       'fontaine-2015-rebuild', 'tennessen-2021', '1338-VO-NG-ADEDAPO-VMF00268']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de710fa4",
   "metadata": {},
   "source": [
    "Then lets select which cohorts we want to load, connect to Ag3() API and load some metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c9424",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define region strings\n",
    "region_dict = dict(zip(region_names, regions))\n",
    "\n",
    "for k, v in region_dict.items():\n",
    "    region_dict[k] = f'{contig}:{v-flanking}-{v+flanking}'\n",
    "\n",
    "ag3 = malariagen_data.Ag3(results_cache=\"../results_cache/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f8e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_samples = ag3.sample_metadata(sample_sets)\n",
    "df_samples['taxon'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf66a9a0",
   "metadata": {},
   "source": [
    "### Write haplotypes to FASTA\n",
    "\n",
    "In this notebook, we will construct FASTA sequences (or a multi-sequence alignment) from haplotypes in selected cohorts of the ag3, plus some outgroups. We will use *An. merus*, *An. melas*, and *An. quadriannulatus* as outgroups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab57b68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def haps_to_fasta_eric(haplos, var_alleles):\n",
    "    return(var_alleles[range(var_alleles.shape[0]), np.transpose(haplos)])\n",
    "\n",
    "\n",
    "def haps_to_fasta_df(haps):\n",
    "    \"\"\"\n",
    "    Take haplotype XArray and create pandas df of fasta sequences\n",
    "    \"\"\"\n",
    "    # transform xarray into haplotype array\n",
    "    haplos = allel.GenotypeArray(haps['call_genotype'].compute()).to_haplotypes().values\n",
    "    # extract ref and alt alleles array\n",
    "    var_alleles = haps['variant_allele'].compute().values.astype(str)\n",
    "        \n",
    "    seq_arr = haps_to_fasta_eric(haplos, var_alleles)\n",
    "    seq_arr = np.vstack(seq_arr)\n",
    "    \n",
    "    # Make dataframe of all haplotype sequences for region\n",
    "    fasta_df = pd.DataFrame(seq_arr)\n",
    "    fasta_df.columns = haps['variant_position'].compute().values\n",
    "    sample_ids = haps['sample_id'].compute().values\n",
    "    fasta_df.loc[:, 'hap'] = [\"> \" + p for p in make_unique(np.repeat(sample_ids, 2))]\n",
    "    cols = list(fasta_df)\n",
    "    # move the column to head of list using index, pop and insert\n",
    "    cols.insert(0, cols.pop(cols.index('hap')))\n",
    "    fasta_df = fasta_df.loc[:, cols]\n",
    "    return(fasta_df)\n",
    "\n",
    "def remove_missing_invariant_fasta_df(fasta_df, remove_invariant=False):\n",
    "    \"\"\"\n",
    "    Remove any columns in pandas dataframe that are missing genotypes ('.')\n",
    "    \"\"\"\n",
    "    # missing_bool = fasta_df.apply(lambda x: any(x == b'.') , axis=0)\n",
    "    # print(f\"Removing {missing_bool.sum()} alleles with a missing call\")\n",
    "    # fasta_df = fasta_df.loc[:, ~missing_bool]\n",
    "    fasta_df = fasta_df.set_index('hap')\n",
    "    \n",
    "    if remove_invariant:\n",
    "        invariant_cols = fasta_df.nunique() <= 1\n",
    "        print(f\"Removing {invariant_cols.sum()} invariant SNPs\")\n",
    "        fasta_df = fasta_df.loc[:, ~invariant_cols]\n",
    "        print(f\"There are {fasta_df.shape[0]} haplotypes and {fasta_df.shape[1]} segregating haplotype calls\")\n",
    "\n",
    "    # More efficient string joining\n",
    "    sequences = [''.join(row) for row in fasta_df.values]\n",
    "    fasta_df = fasta_df.copy()  # De-fragment first\n",
    "    fasta_df['seq'] = sequences\n",
    "    \n",
    "    return(fasta_df.reset_index()[['hap', 'seq']])\n",
    "\n",
    "\n",
    "def make_unique(values):\n",
    "    value_counts = {}\n",
    "    unique_values = []\n",
    "    \n",
    "    for value in values:\n",
    "        if value in value_counts:\n",
    "            value_counts[value] += 1\n",
    "            unique_values.append(f\"{value}_{value_counts[value]}\")\n",
    "        else:\n",
    "            value_counts[value] = 0\n",
    "            unique_values.append(f\"{value}_{value_counts[value]}\")\n",
    "    \n",
    "    return np.array(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45182b03",
   "metadata": {},
   "source": [
    "For each region, convert the haps to fasta and write to file! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2286b064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "haps = {}\n",
    "for name, region in region_dict.items():\n",
    "    # Load haplotypes for region, find intersection with phase 1 outgroup data\n",
    "    print(f\"loading haplotypes | {name} | {region}\")\n",
    "    haps[name] = ag3.haplotypes(region=region, sample_sets=sample_sets, analysis='gamb_colu_arab')\n",
    "    print(name, region, haps[name]['call_genotype'].shape)\n",
    "    sample_ids = haps[name]['sample_id'].compute().values\n",
    "    \n",
    "    # for each haplotype, loop through SNPs and create a FASTA sequence array depending on alleles\n",
    "    print(f\"Converting haps to FASTA sequence | {name}\")\n",
    "    fasta_df = haps_to_fasta_df(haps[name])\n",
    "    \n",
    "    # remove missing alleles in the outgroups and invariant sites \n",
    "    fasta_df = remove_missing_invariant_fasta_df(fasta_df, remove_invariant=False)\n",
    "    \n",
    "    # write to csv with \\n sep to make FASTA file\n",
    "    fasta_df.to_csv(f\"{out_prefix}/{locus_name}_{name}.fasta\", sep=\"\\n\", index=False, header=False)\n",
    "    print(f'Multiple alignment FASTA written \\n')\n",
    "    \n",
    "    df_samples = ag3.sample_metadata().set_index('sample_id')\n",
    "    df_samples = df_samples.loc[sample_ids, :].reset_index()\n",
    "    \n",
    "    # Create haplotype version with consecutive haplotypes per mosquito\n",
    "    df_samples_haps = df_samples.loc[df_samples.index.repeat(2)].reset_index(drop=True)\n",
    "    # Create the sample_id column to match the fasta files\n",
    "    df_samples_haps['sample_id'] = make_unique(np.repeat(df_samples['sample_id'].values, 2))    \n",
    "    \n",
    "    # remove > and join with metadata for each pop, useful for plotting phylo trees metadata\n",
    "    fasta_df.loc[:, 'hap'] = fasta_df['hap'].str.strip('> ')\n",
    "    all_haps = pd.concat([fasta_df.reset_index(drop=True), df_samples_haps.reset_index(drop=True)], axis=1)\n",
    "    all_haps.to_csv(f\"{out_prefix}/{locus_name}_{name}.metadata.tsv\", sep=\"\\t\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d20168",
   "metadata": {},
   "source": [
    "## Running IQTree\n",
    "You may prefer to do this outside of the jupyter notebook, it can take a while :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc1124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for name in names:\n",
    "#   !iqtree -s {locus_name}_{name}.fasta -B 10000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
